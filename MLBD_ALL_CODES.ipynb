{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLBD_ALL_CODES.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bLVczEd3PF1Q",
        "8kbouSJuh2U5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithesh-shettigar/ml-exam/blob/main/MLBD_ALL_CODES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K - Means Clustering"
      ],
      "metadata": {
        "id": "bLVczEd3PF1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w7JzuklhPBzz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "0cd58be2-d09d-4afc-cd6c-395ffc0ef9b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-458695e87993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Countryclusters.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Countryclusters.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "data = pd.read_csv('Countryclusters.csv')\n",
        "data\n",
        "\n",
        "x = data.iloc[:,1:3]\n",
        "x\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "X['status_type'] = le.fit_transform(X['status_type'])\n",
        "y = le.transform(y)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)  #RANDOMLY TAKING CLUSTERS\n",
        "kmeans.fit(X)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "cs = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0) #ELBOW METHOD FOR CHOOSING K\n",
        "    kmeans.fit(X)\n",
        "    cs.append(kmeans.inertia_)\n",
        "plt.plot(range(1, 11), cs)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('CS')\n",
        "plt.show()\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2,random_state=0)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "# check how many of the samples were correctly labeled\n",
        "correct_labels = sum(y == labels)\n",
        "print(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n",
        "print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM CLASSIFIER"
      ],
      "metadata": {
        "id": "1fwLiRZfYlKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "bankdata = pd.read_csv(\"/content/bill_authentication.csv\")\n",
        "\n",
        "bankdata.shape\n",
        "bankdata.head()\n",
        "\n",
        "X = bankdata.drop('Class', axis=1)\n",
        "y = bankdata['Class']\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "svclassifier = SVC(kernel='linear')\n",
        "svclassifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svclassifier.predict(X_test)\n",
        "print(y_pred)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "ax = plt.subplot()\n",
        "ax.scatter(bankdata[bankdata['Class'] == 1]['Variance'], bankdata[bankdata['Class'] == 1]['Skewness'], c='green', s=bankdata[bankdata['Class'] == 1]['Curtosis'])\n",
        "ax.scatter(bankdata[bankdata['Class'] == 0]['Variance'], bankdata[bankdata['Class'] == 0]['Skewness'], c='red', s=bankdata[bankdata['Class'] == 0]['Curtosis']);"
      ],
      "metadata": {
        "id": "Y2e3sjToYqrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN - Diabetes"
      ],
      "metadata": {
        "id": "8kbouSJuh2U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "df1=pd.read_csv('diabetes.csv')\n",
        "df1\n",
        "x = df1.iloc[:, 0:-1].values\n",
        "y = df1.iloc[:, -1].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train= sc.fit_transform(x_train)\n",
        "x_test= sc.transform(x_test)\n",
        "\n",
        "ann =tf.keras.models.Sequential()\n",
        "ann.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "ann.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
        "\n",
        "ann.fit(x_train, y_train, batch_size = 50, epochs = 100)\n",
        "\n",
        "y_pred = ann.predict(x_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred = y_pred.astype(int)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cn = confusion_matrix(y_test, y_pred)\n",
        "print(cn)\n",
        "\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "2ugDF6uoiAC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN- Bank Modelling"
      ],
      "metadata": {
        "id": "954oje7Yjd2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('/content/Bank_Modelling.csv')\n",
        "print(df)\n",
        "\n",
        "X = df.iloc[:,3:13].values\n",
        "Y = df.iloc[:,13].values\n",
        "print(X)\n",
        "print(Y)\n",
        "\n",
        "#Encoding Categorical data\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "X[:,1] = labelencoder_X_1.fit_transform(X[:,1])\n",
        "X\n",
        "\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "X[:,2] = labelencoder_X_2.fit_transform(X[:,2])\n",
        "X\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "ct = ColumnTransformer([(\"Country\", OneHotEncoder(),[1])], remainder = \"passthrough\")\n",
        "X = ct.fit_transform(X)\n",
        "#X.astype(int)\n",
        "X \n",
        "\n",
        "X = X[:, 1:]\n",
        "X\n",
        "\n",
        "from pandas.core.common import random_state\n",
        "#Splitting the dataset into training and testing set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 0)\n",
        "\n",
        "print(X_train)\n",
        "print('-------------')\n",
        "print(X_test)\n",
        "\n",
        "#Feature Scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc=StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "import tensorflow as tf\n",
        "ann =tf.keras.models.Sequential()\n",
        "ann.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "ann.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
        "\n",
        "ann.fit(X_train, y_train, batch_size = 50, epochs = 100)\n",
        "\n",
        "y_pred = ann.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "y_pred = y_pred.astype(int)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cn = confusion_matrix(y_test, y_pred)\n",
        "print(cn)\n",
        "\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "2oD8lNxnjncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PERCEPTRON"
      ],
      "metadata": {
        "id": "e8_Qs29BlOeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class perceptron:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,inputCount : int, alpha: int):\n",
        "        self.W = np.zeros(inputCount, dtype=int)\n",
        "        self.bias = 0\n",
        "        self.alpha = alpha\n",
        "\n",
        "\n",
        "    # Activation function for and, or and not \n",
        "    @staticmethod\n",
        "    def __activationFunction(activation: int):\n",
        "        #Return 1 if weighted input is > than 0, else return -1\n",
        "        if(activation > 0):\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    \n",
        "    \n",
        "    # Training the model\n",
        "\n",
        "    #Single iteration of training\n",
        "    def __train(self,input : list, target : list):\n",
        "        for x,y in zip(input,target):\n",
        "            weightedInput = np.dot(x,self.W) + self.bias\n",
        "            print(f\"Weighted Input for {x} with weights {self.W} is {weightedInput}\")\n",
        "            \n",
        "            prediction = self.__activationFunction(weightedInput)\n",
        "            print(f\"Target : {y}  Prediction : {prediction}\")\n",
        "\n",
        "            #Modify weights if predicted value != target  \n",
        "            \n",
        "            if (prediction != y):\n",
        "                print(\"Target and Predicted value don't match. Modify the weights.\")\n",
        "                #Modify weights\n",
        "                self.W = self.W + (self.alpha * y * x)\n",
        "                #Modify Bias\n",
        "                self.bias = self.bias + (self.alpha * y) \n",
        "                print(f\"Updated Weights : {self.W}, Updated Bias : {self.bias}\")\n",
        "\n",
        "            print(\"\\n\")\n",
        "\n",
        "    #Method to train the model         \n",
        "    def fit(self, input : list, target : list, ephochs : int):\n",
        "        for i in range(ephochs+1):\n",
        "            print(f\"Epoch {i}\")\n",
        "            self.__train(input,target)\n",
        "\n",
        "    \n",
        "    #predicts output for the given input, input should be in the form of list\n",
        "    def predict(self,input : list):\n",
        "        return self.__activationFunction(np.dot(input,self.W) + self.bias)\n",
        "    \n",
        "#And Gate\n",
        "\n",
        "#Create an instance of perceptron class \n",
        "andPerceptron = perceptron(2,1)\n",
        "\n",
        "#Training set\n",
        "X =  np.array([[1,1],[1,-1],[-1,1],[-1,-1]]) #Inputs\n",
        "Y = [1,-1,-1,-1] #Target\n",
        "\n",
        "#Training the model -> Number of ephochs = 3\n",
        "andPerceptron.fit(X,Y,2)\n",
        "\n",
        "#testing the model\n",
        "test = np.array([[-1,-1],[-1,1],[1,-1],[1,1]])\n",
        "print(f\"Output for input {test[0]} is {andPerceptron.predict(test[0])}\")\n",
        "print(f\"Output for input {test[1]} is {andPerceptron.predict(test[1])}\")\n",
        "print(f\"Output for input {test[2]} is {andPerceptron.predict(test[2])}\")\n",
        "print(f\"Output for input {test[3]} is {andPerceptron.predict(test[3])}\")\n",
        "\n",
        "#-------------------------------GRAPH------------------------\n",
        "\n",
        "X1 = np.array([[-1, -1], [1, -1], [1, 1], [-1, 1]], np.float32) # 4x2, input\n",
        "Y1 = np.array([-1, -1, 1, -1], np.float32) # 4, correct output, AND operation\n",
        "W = andPerceptron.W\n",
        "b = andPerceptron.bias\n",
        "plot_x = np.array([-2,2])\n",
        "plot_y = - 1 / W[1] * (W[0] * plot_x + b) # comes from, w0*x + w1*y + b = 0 then y = (-1/w1) (w0*x + b)\n",
        "\n",
        "plt.scatter(X1[:, 0], X1[:, 1], c=Y1, s=100, cmap='viridis')\n",
        "plt.plot(plot_x, plot_y, color='k', linewidth=2)\n",
        "plt.xlim([-2, 2]); plt.ylim([-2, 2]);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SA0afJjglVtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q LEARNING"
      ],
      "metadata": {
        "id": "RPFETy3tl8W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#-------\n",
        "points_list = [(0,4),(4,3),(3,2),(3,1),(1,5),(4,5)]\n",
        "goal = 5\n",
        "#------\n",
        "import networkx as nx\n",
        "G = nx.Graph()\n",
        "G.add_edges_from(points_list)\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw_networkx_nodes(G,pos)\n",
        "nx.draw_networkx_labels(G,pos)\n",
        "nx.draw_networkx_edges(G,pos)\n",
        "plt.show()\n",
        "#------\n",
        "R = np.array([[-1,-1,-1,-1,0,-1],[-1,-1,-1,0,-1,100],[-1,-1,-1,0,-1,-1],[-1,0,0,-1,0,-1],[0,-1,-1,0,-1,100],[-1,0,-1,-1,0,100]])\n",
        "print(R)\n",
        "#------\n",
        "Q = np.zeros([6,6])\n",
        "print(Q)\n",
        "#-------\n",
        "#Set learning parameter gamma\n",
        "gamma = 0.8\n",
        "#Initial state(ususally to be chosen at random)\n",
        "initial_state = 1\n",
        "#--------\n",
        "def available_actions(state):\n",
        "  current_state_row = R[state, :]\n",
        "  av_act = np.where(current_state_row >= 0)[0]\n",
        "  return av_act\n",
        "\n",
        "available_act = available_actions(initial_state)\n",
        "#-----\n",
        "def sample_next_action(available_actions_range):\n",
        "  next_action = int(np.random.choice(available_act,1))\n",
        "  return next_action\n",
        "\n",
        "action = sample_next_action(available_act)\n",
        "#------\n",
        "def update(current_state,action,gamma):\n",
        "  max_index = np.where(Q[action,] == np.max(Q[action, :]))[0]\n",
        "\n",
        "  if max_index.shape[0] > 1:\n",
        "    max_index = int(np.random.choice(max_index,size=1))\n",
        "  else:\n",
        "    max_index = int(max_index)\n",
        "  max_value = Q[action,max_index]\n",
        "\n",
        "  Q[current_state,action] = R[current_state,action] + gamma * max_value\n",
        "#--------\n",
        "update(initial_state,action,gamma)\n",
        "#------\n",
        "for i in range(10000):\n",
        "  current_state = np.random.randint(0,int(Q.shape[0]))\n",
        "  available_act = available_actions(current_state)\n",
        "  action = sample_next_action(available_act)\n",
        "  update(current_state,action,gamma)\n",
        "\n",
        "#-------\n",
        "\n",
        "#Normalise trained Q matrix\n",
        "print(\"Trained Q matrix: \")\n",
        "print(Q/np.max(Q)*100)\n",
        "\n",
        "#------\n",
        "\n",
        "#testing\n",
        "\n",
        "#Goal_state = 5\n",
        "# Testing\n",
        "current_state = 2\n",
        "steps = [current_state]\n",
        "  \n",
        "while current_state != 5:\n",
        "  \n",
        "    next_step_index = np.where(Q[current_state, :] == np.max(Q[current_state, :]))[0]\n",
        "    \n",
        "    if next_step_index.shape[0] > 1:\n",
        "        next_step_index = int(np.random.choice(next_step_index, size = 1))\n",
        "    else:\n",
        "        next_step_index = int(next_step_index)\n",
        "    steps.append(next_step_index)\n",
        "    current_state = next_step_index\n",
        "  \n",
        "print(\"Most efficient path:\")\n",
        "print(steps)"
      ],
      "metadata": {
        "id": "admXysUDmEVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UCB_RIL"
      ],
      "metadata": {
        "id": "std6aRAenZCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('/content/Ads_CTR_Optimisation.csv')\n",
        "dataset\n",
        "\n",
        "#--------\n",
        "\n",
        "# Implementing UCB\n",
        "import math\n",
        "N = 10000\n",
        "d = 10\n",
        "ads_selected = []\n",
        "numbers_of_selections = [0] * d\n",
        "sums_of_reward = [0] * d\n",
        "total_reward = 0\n",
        "\n",
        "for n in range(0, N):\n",
        "    ad = 0\n",
        "    max_upper_bound = 0\n",
        "    for i in range(0, d):\n",
        "        if (numbers_of_selections[i] > 0):\n",
        "            average_reward = sums_of_reward[i] / numbers_of_selections[i]\n",
        "            delta_i = math.sqrt(3/2 * math.log(n+1) / numbers_of_selections[i])\n",
        "            upper_bound = average_reward + delta_i\n",
        "        else:\n",
        "            upper_bound = 1e400 #SETTING HIGH BOUNDARY\n",
        "        if upper_bound > max_upper_bound:\n",
        "            max_upper_bound = upper_bound\n",
        "            ad = i\n",
        "    ads_selected.append(ad)\n",
        "    numbers_of_selections[ad] += 1\n",
        "    reward = dataset.values[n, ad]\n",
        "    sums_of_reward[ad] += reward\n",
        "    total_reward = total_reward + reward\n",
        "\n",
        "#------\n",
        "\n",
        "pd.Series(ads_selected).head(1050).value_counts(normalize=True)\n",
        "print(ad)\n",
        "print(reward)\n",
        "#----\n",
        "plt.hist(ads_selected)\n",
        "print(total_reward)"
      ],
      "metadata": {
        "id": "-AH_M4xQndBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "dppxIM4ynyKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image \n",
        "from glob import glob\n",
        "\n",
        "train_path = '../input/fruits/fruits-360/Training/'\n",
        "test_path = '../input/fruits/fruits-360/Test/'\n",
        "\n",
        "img = load_img(train_path + \"Apple Braeburn/0_100.jpg\", target_size=(100,100))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "images = ['Orange', 'Banana', 'Cauliflower', 'Cactus fruit', 'Eggplant', 'Avocado', 'Blueberry','Lemon', 'Kiwi']\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig = plt.figure(figsize =(15,5))\n",
        "for i in range(9):\n",
        "    ax = fig.add_subplot(3,3,i+1,xticks=[],yticks=[])\n",
        "    #fig.patch.set_facecolor('#E53090')\n",
        "    #Above code adds a background color for subplots you can change the hex color code as you wish\n",
        "    plt.title(images[i])\n",
        "    plt.axis(\"off\")\n",
        "    ax.imshow(load_img(train_path + images[i] +\"/0_100.jpg\", target_size=(100,100)))\n",
        "\n",
        "x = img_to_array(img)\n",
        "print(x.shape)\n",
        "\n",
        "className = glob(train_path + '/*')\n",
        "number_of_class = len(className)\n",
        "print(number_of_class)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), input_shape= x.shape))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(32, (3,3),))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(64, (3,3),))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(number_of_class))#output\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\",\n",
        "             optimizer = \"rmsprop\",\n",
        "             metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "qODi7KYwn16P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}